{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Összefűzzük a dialógusokat. Létrehozunk egy file-t, amely az összes dialógust tartalmazza és egy meta file-t, hogy vissza tudjuk keresni a dialógusokat."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import huspacy\n",
    "import os\n",
    "import pickle\n",
    "directory = \"../data/combined_transcripts_freeConv\"\n",
    "\n",
    "docs = []\n",
    "meta = {}\n",
    "i = 0\n",
    "for filename in os.listdir(directory):\n",
    "    with open(os.path.join(directory, filename), 'r', encoding=\"UTF-8\") as f:\n",
    "        txt = f.read()\n",
    "\n",
    "    mordor = [e.strip() for e in txt.split(\"\\n\") if e.startswith(\"Mordor: \")]\n",
    "    gondor = [e.strip() for e in txt.split(\"\\n\") if e.startswith(\"Gondor: \")]\n",
    "\n",
    "    j = len(mordor)\n",
    "    k = len(gondor)\n",
    "    for h in range(j):\n",
    "        meta[i+h] = filename + \"_mordor_\" + str(h)\n",
    "    for g in range(k):\n",
    "        meta[i+g+j] = filename + \"_gondor_\" + str(g)\n",
    "    mordor = [e.replace(\"Mordor: \", \"\") for e in mordor]\n",
    "    gondor = [e.replace(\"Gondor: \", \"\") for e in gondor]\n",
    "\n",
    "    docs += mordor + gondor\n",
    "    a = (j+k)\n",
    "    i += a\n",
    "with open(\"../resources/docs.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(docs, outfile)\n",
    "\n",
    "with open(\"../resources/meta.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(meta, outfile)\n",
    "\n",
    "print(\"Az adatbázis\",len(docs), \"beszédfordulót tartalmaz.\")\n",
    "print(\"Az adatbázis első beszédfordulójának metaadata:\", meta[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-15T10:29:34.523508Z",
     "start_time": "2024-11-15T10:29:34.414669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Az adatbázis 23604 beszédfordulót tartalmaz.\n",
      "Az adatbázis első beszédfordulójának metaadata: pair154_freeConv_combined_srt_files.txt_mordor_0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Előfeldolgozzuk a zsöveget: lemmatizálunk, tokenizálunk, POS-tagelünk. Lementjük az adatokat. Lehet, hogy a huspacy nem fog betölteni, akkor használjuk az előre megadott lemmatizált szöveget és menjünk tovább az ebben a notebookban lévő 3. lépésre."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "import huspacy\n",
    "nlp = huspacy.load()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-15T10:29:34.745690Z",
     "start_time": "2024-11-15T10:29:34.588452Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mhuspacy\u001B[39;00m\n\u001B[1;32m      3\u001B[0m nlp \u001B[38;5;241m=\u001B[39m huspacy\u001B[38;5;241m.\u001B[39mload()\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'spacy'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "docs_lemmatized = []\n",
    "docs_pos = []\n",
    "docs_tokenized = []\n",
    "\n",
    "for line in docs:\n",
    "    lemmatized = []\n",
    "    postagged = []\n",
    "    tokenized = []\n",
    "    processed_line = nlp(line)\n",
    "    for token in processed_line:\n",
    "        if token.pos_ not in [\"PUNCT\"]:\n",
    "            lemma = token.lemma_.replace(\">\",\"\").replace(\".\",\"\").lower()\n",
    "            pos = f\"{lemma}_{token.pos_}\"\n",
    "            str_token = token.text\n",
    "            lemmatized.append(lemma)\n",
    "            tokenized.append(str_token)\n",
    "            postagged.append(pos)\n",
    "    if len(lemmatized) == 0:\n",
    "        lemmatized = [\"U\"]\n",
    "    if len(postagged) == 0:\n",
    "        postagged = [\"U\"]\n",
    "    if len(tokenized) == 0:\n",
    "        tokenized = [\"U\"]\n",
    "    docs_lemmatized.append(\" \".join(lemmatized))\n",
    "    docs_pos.append(\" \".join(postagged))\n",
    "    docs_tokenized.append(\" \".join(tokenized))\n",
    "\n",
    "with open(\"../resources/lemmatized.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(docs_lemmatized, outfile)\n",
    "\n",
    "with open(\"../resources/pos.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(docs_pos, outfile)\n",
    "\n",
    "with open(\"../resources/tokenized.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(docs_tokenized, outfile)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Folytatjuk az előfeldolgozást: kiszűrjük a stop szavakat, lementjük az adatot."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "lemmatized = pickle.load(open(\"../data/resources.pkl\", \"rb\"))\n",
    "with open('../resources/stopwordlist.txt', 'r') as infile:\n",
    "    stoplist = infile.read().split('\\n')\n",
    "lemmatized_split = [e.split()for e in lemmatized]\n",
    "filtered = []\n",
    "for e in lemmatized_split:\n",
    "    temp = []\n",
    "    for word in e:\n",
    "        if word not in stoplist:\n",
    "            temp.append(word)\n",
    "    if len(temp) > 0:\n",
    "        filtered.append(\" \".join(temp))\n",
    "    else:\n",
    "        filtered.append(\"PlaceHolder\")\n",
    "with open(\"../resources/no_stopword.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(filtered, outfile)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-15T10:29:34.791052004Z",
     "start_time": "2024-10-03T14:13:40.007556854Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
