{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### A BERTopic-ról hasznos dokumentumok\n",
    "https://maartengr.github.io/BERTopic/algorithm/algorithm.html#6-optional-fine-tune-topic-representation\n",
    "\n",
    "https://towardsdatascience.com/topics-per-class-using-bertopic-252314f2640\n",
    "\n",
    "https://people.inf.elte.hu/csa/html/szinek.htm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import pickle\n",
    "import huspacy\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-15T11:02:28.841667Z",
     "start_time": "2024-11-15T11:02:19.845760Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Beolvassuk az előfeldolgozott korpuszokat. Később választhatunk, hogy a lemmatizált vagy a lemmatizált és stop szavaktól megszűrt korpuszon dolgozunk-e."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "meta = pickle.load(open(\"../resources/meta.pkl\", \"rb\"))\n",
    "lemmatized = pickle.load(open(\"../resources/lemmatized.pkl\", \"rb\"))\n",
    "#pos = pickle.load(open(\"../resources/pos.pkl\", \"rb\"))\n",
    "#tokens =  pickle.load(open(\"../resources/tokenized.pkl\", \"rb\"))\n",
    "#doc_stop = pickle.load(open(\"../resources/no_stopword.pkl\", \"rb\")) ### kevesebb stopszóval\n",
    "doc_stop_2 = pickle.load(open(\"../resources/stopword_filtered.pkl\", \"rb\")) ### több stop szóval, a no_stopword.pkl kiegészítve\n",
    "all_docs = pickle.load(open(\"../resources/docs.pkl\", \"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-15T10:56:27.532504Z",
     "start_time": "2024-11-15T10:56:27.497340Z"
    }
   },
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Készítünk egy listát az előre megadott témák szavaiból"
  },
  {
   "cell_type": "code",
   "source": [
    "seed_topic_list = [[\"szabadidő\", \"szabadidőtök\", \"szabadidőd\", \"szabadidődet\"],\n",
    "                   [\"nyelv\", \"nyelvtanulás\", \"nyelvvizsga\"],\n",
    "                   [\"sport\", \"sportol\", \"sportolás\"],\n",
    "                   [\"ismerkedés\", 'megismerkedik', \"megismer\"],\n",
    "                   [\"olvas\", \"olvasás\", \"könyv\"],\n",
    "                   ['külföld', 'külföldi', 'utazik'],\n",
    "                   [\"magyarország\"],\n",
    "                   [\"social\", \"media\", \"facebook\", \"facebookon\", 'instagram', 'instagramm', 'instagrammom', 'instagrammon'], [\"igen\", \"ja\", \"persze\", \"aha\", \"hum\"]]\n",
    "#, [\"laugh\", \"nevet\", \"vicces\"], [\"placeholder\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-15T10:56:27.595019Z",
     "start_time": "2024-11-15T10:56:27.592163Z"
    }
   },
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "source": [
    "## 3. Topic modellezünk. Beadjuk seednek az előre megadott témákat, így azokat könnyebben azonosítja a modell. Lekérjük minden topik leggyakoribb 40 szavát. Eldöntjük, hogy a lemmatizált vagy a stop szavazott adaton dolgozunk-e. 3 féle modellt kipróbálunk."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-15T10:56:27.713118Z",
     "start_time": "2024-11-15T10:56:27.710792Z"
    }
   },
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Modell: BERTopic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
    "\n",
    "key = KeyBERTInspired()\n",
    "mm =  MaximalMarginalRelevance(diversity=0.3)\n",
    "btm = BERTopic(\"hungarian\", representation_model=[key, mm], seed_topic_list=seed_topic_list, min_topic_size = 50, calculate_probabilities=True)\n",
    "topics, probs = btm.fit_transform(doc_stop_2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-15T10:56:30.694483Z",
     "start_time": "2024-11-15T10:56:27.757386Z"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 368.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 384.00 KiB is free. Process 4938 has 1011.00 MiB memory in use. Including non-PyTorch memory, this process has 981.00 MiB memory in use. Of the allocated memory 918.39 MiB is allocated by PyTorch, and 25.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m mm \u001B[38;5;241m=\u001B[39m  MaximalMarginalRelevance(diversity\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m)\n\u001B[1;32m      6\u001B[0m btm \u001B[38;5;241m=\u001B[39m BERTopic(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhungarian\u001B[39m\u001B[38;5;124m\"\u001B[39m, representation_model\u001B[38;5;241m=\u001B[39m[key, mm], seed_topic_list\u001B[38;5;241m=\u001B[39mseed_topic_list, min_topic_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m50\u001B[39m, calculate_probabilities\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 7\u001B[0m topics, probs \u001B[38;5;241m=\u001B[39m \u001B[43mbtm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc_stop_2\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/bertopic/_bertopic.py:430\u001B[0m, in \u001B[0;36mBERTopic.fit_transform\u001B[0;34m(self, documents, embeddings, images, y)\u001B[0m\n\u001B[1;32m    428\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m embeddings \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    429\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmbedding - Transforming documents to embeddings.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 430\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_model \u001B[38;5;241m=\u001B[39m \u001B[43mselect_backend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlanguage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    431\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extract_embeddings(\n\u001B[1;32m    432\u001B[0m         documents\u001B[38;5;241m.\u001B[39mDocument\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mtolist(),\n\u001B[1;32m    433\u001B[0m         images\u001B[38;5;241m=\u001B[39mimages,\n\u001B[1;32m    434\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    435\u001B[0m         verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[1;32m    436\u001B[0m     )\n\u001B[1;32m    437\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmbedding - Completed \u001B[39m\u001B[38;5;130;01m\\u2713\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/bertopic/backend/_utils.py:135\u001B[0m, in \u001B[0;36mselect_backend\u001B[0;34m(embedding_model, language, verbose)\u001B[0m\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SentenceTransformerBackend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m language\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;129;01min\u001B[39;00m languages \u001B[38;5;129;01mor\u001B[39;00m language \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultilingual\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSentenceTransformerBackend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    138\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlanguage\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is currently not supported. However, you can \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    139\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcreate any embeddings yourself and pass it through fit_transform(docs, embeddings)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    140\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mElse, please select a language from the following list:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    141\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlanguages\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    142\u001B[0m     )\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/bertopic/backend/_sentencetransformers.py:44\u001B[0m, in \u001B[0;36mSentenceTransformerBackend.__init__\u001B[0;34m(self, embedding_model)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_model \u001B[38;5;241m=\u001B[39m embedding_model\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(embedding_model, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m---> 44\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_model \u001B[38;5;241m=\u001B[39m \u001B[43mSentenceTransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hf_model \u001B[38;5;241m=\u001B[39m embedding_model\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:345\u001B[0m, in \u001B[0;36mSentenceTransformer.__init__\u001B[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001B[0m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    343\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m--> 345\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_hpu_graph_enabled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_prompt_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_prompt_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprompts:\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1340\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1337\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1338\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m-> 1340\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    898\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    899\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 900\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    902\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    903\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    904\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    905\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    910\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    911\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    898\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    899\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 900\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    902\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    903\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    904\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    905\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    910\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    911\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[0;31m[... skipping similar frames: Module._apply at line 900 (1 times)]\u001B[0m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    898\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    899\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 900\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    902\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    903\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    904\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    905\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    910\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    911\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:927\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    923\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    924\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    925\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    926\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 927\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    928\u001B[0m p_should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    930\u001B[0m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1326\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m   1319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m   1320\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n\u001B[1;32m   1321\u001B[0m             device,\n\u001B[1;32m   1322\u001B[0m             dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1323\u001B[0m             non_blocking,\n\u001B[1;32m   1324\u001B[0m             memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format,\n\u001B[1;32m   1325\u001B[0m         )\n\u001B[0;32m-> 1326\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1327\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1328\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1329\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1330\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1331\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1332\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot copy out of meta tensor; no data!\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 368.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 384.00 KiB is free. Process 4938 has 1011.00 MiB memory in use. Including non-PyTorch memory, this process has 981.00 MiB memory in use. Of the allocated memory 918.39 MiB is allocated by PyTorch, and 25.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": [
    "btm_jo_seed_ti= btm.get_topic_info()\n",
    "btm_jo_seed_ti"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Reduce outliers. Kétféle módszert próbálunk ki: valószínűség és eloszlás alapján.",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Valószínűség alapján"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "new_topics = btm.reduce_outliers(doc_stop_2, topics, probabilities=probs, strategy=\"probabilities\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Frissítjük a modellt az outlierek kizárása után létrehozott új topikokkal és topikeloszlásokkal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "btm.update_topics(doc_stop_2, new_topics)\n",
    "documents = pd.DataFrame({\"Document\": doc_stop_2, \"Topic\": new_topics})\n",
    "btm._update_topic_size(documents)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "btm_probs_ti = btm.get_topic_info()\n",
    "btm_probs_ti"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "probs_df_2=pd.DataFrame(probs)\n",
    "probs_df_2['main percentage'] = pd.DataFrame({'max': probs_df_2.max(axis=1)})\n",
    "probs_df_2"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Eloszlás alapján (default)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
    "\n",
    "key = KeyBERTInspired()\n",
    "mm =  MaximalMarginalRelevance(diversity=0.3)\n",
    "sima_btm = BERTopic(\"hungarian\", representation_model=[key, mm], seed_topic_list=seed_topic_list, min_topic_size = 50, calculate_probabilities=True)\n",
    "topics, probs = sima_btm.fit_transform(doc_stop_2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "new_topics_2 = sima_btm.reduce_outliers(doc_stop, topics)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "sima_btm.update_topics(doc_stop, new_topics_2)\n",
    "documents_2 = pd.DataFrame({\"Document\": doc_stop, \"Topic\": new_topics_2})\n",
    "sima_btm._update_topic_size(documents_2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "btm_sima_ti_new = sima_btm.get_topic_info()\n",
    "btm_sima_ti_new"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Modell: Sentence Transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sentence_model = SentenceTransformer(\"NYTK/sentence-transformers-experimental-hubert-hungarian\")\n",
    "sentence_transformer_lemmatized = BERTopic(embedding_model=sentence_model, min_topic_size = 30, seed_topic_list=seed_topic_list)\n",
    "topics, probs = sentence_transformer_lemmatized.fit_transform(lemmatized)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "sentence_topic_info_lemmatized = sentence_transformer_lemmatized.get_topic_info()\n",
    "sentence_topic_info_lemmatized"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Modell: Huspacy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "nlp = huspacy.load()\n",
    "spacy_lemmatized = BERTopic(embedding_model=nlp, min_topic_size = 30, seed_topic_list=seed_topic_list)\n",
    "topics, probs = spacy_lemmatized.fit_transform(lemmatized)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "spacy_topic_info_lemmatized = spacy_lemmatized.get_topic_info()\n",
    "spacy_topic_info_lemmatized"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 HDBscan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "hdbscan_model = HDBSCAN(min_cluster_size=30, metric='euclidean', cluster_selection_method='eom', prediction_data=True, min_samples=10)\n",
    "hdbscan_lemmatized = BERTopic(hdbscan_model=hdbscan_model, seed_topic_list=seed_topic_list)\n",
    "topics, probs = hdbscan_lemmatized.fit_transform(lemmatized)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "hdbscan_lemmatized_topic_info = hdbscan_lemmatized.get_topic_info()\n",
    "hdbscan_lemmatized_topic_info"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Lementjük a modelleket és a topikokat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def save_model(model_name, model_path):\n",
    "    model_name.save(model_path, serialization=\"pickle\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "save_model(btm,\"../models/bert_model_probabilities.pkl\")",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def save_topic_info(model_topic_info, topic_path):\n",
    "    model_topic_info.to_csv(topic_path, sep=\",\", index=False, encoding=\"UTF-8\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "save_topic_info(btm_probs_ti, \"../results/bert_model_probabilities.csv\")",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Összevonjuk a hasonló topikokat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Először betöltjük a használni kívánt modellt. Jelen esetben azt a BERTopic modellt töltjük vissza, amelynél a valószínűség alapján csökkentettök az outliereket.",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "loaded_model_path = \"../models/bert_model_probabilities.pkl\"\n",
    "model = BERTopic.load(loaded_model_path)\n",
    "print(\"A modellt betöltöttük\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-15T10:57:02.394414Z",
     "start_time": "2024-11-15T10:57:01.747111Z"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 368.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 384.00 KiB is free. Process 4938 has 1011.00 MiB memory in use. Including non-PyTorch memory, this process has 981.00 MiB memory in use. Of the allocated memory 918.40 MiB is allocated by PyTorch, and 25.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m loaded_model_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../models/bert_model_probabilities.pkl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mBERTopic\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloaded_model_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA modellt betöltöttük\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/bertopic/_bertopic.py:3387\u001B[0m, in \u001B[0;36mBERTopic.load\u001B[0;34m(cls, path, embedding_model)\u001B[0m\n\u001B[1;32m   3385\u001B[0m             topic_model\u001B[38;5;241m.\u001B[39membedding_model \u001B[38;5;241m=\u001B[39m select_backend(embedding_model, verbose\u001B[38;5;241m=\u001B[39mtopic_model\u001B[38;5;241m.\u001B[39mverbose)\n\u001B[1;32m   3386\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3387\u001B[0m             topic_model \u001B[38;5;241m=\u001B[39m \u001B[43mjoblib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3388\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m topic_model\n\u001B[1;32m   3390\u001B[0m \u001B[38;5;66;03m# Load from directory or HF\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/joblib/numpy_pickle.py:648\u001B[0m, in \u001B[0;36mload\u001B[0;34m(filename, mmap_mode)\u001B[0m\n\u001B[1;32m    646\u001B[0m     filename \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(fobj, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    647\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _read_fileobject(fobj, filename, mmap_mode) \u001B[38;5;28;01mas\u001B[39;00m fobj:\n\u001B[0;32m--> 648\u001B[0m         obj \u001B[38;5;241m=\u001B[39m \u001B[43m_unpickle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    649\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    650\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(filename, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/joblib/numpy_pickle.py:577\u001B[0m, in \u001B[0;36m_unpickle\u001B[0;34m(fobj, filename, mmap_mode)\u001B[0m\n\u001B[1;32m    575\u001B[0m obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 577\u001B[0m     obj \u001B[38;5;241m=\u001B[39m \u001B[43munpickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    578\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m unpickler\u001B[38;5;241m.\u001B[39mcompat_mode:\n\u001B[1;32m    579\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe file \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m has been generated with a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    580\u001B[0m                       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjoblib version less than 0.10. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    581\u001B[0m                       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease regenerate this pickle file.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    582\u001B[0m                       \u001B[38;5;241m%\u001B[39m filename,\n\u001B[1;32m    583\u001B[0m                       \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n",
      "File \u001B[0;32m/usr/lib/python3.12/pickle.py:1205\u001B[0m, in \u001B[0;36m_Unpickler.load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1203\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m\n\u001B[1;32m   1204\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, bytes_types)\n\u001B[0;32m-> 1205\u001B[0m         \u001B[43mdispatch\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1206\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _Stop \u001B[38;5;28;01mas\u001B[39;00m stopinst:\n\u001B[1;32m   1207\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m stopinst\u001B[38;5;241m.\u001B[39mvalue\n",
      "File \u001B[0;32m/usr/lib/python3.12/pickle.py:1582\u001B[0m, in \u001B[0;36m_Unpickler.load_reduce\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1580\u001B[0m args \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39mpop()\n\u001B[1;32m   1581\u001B[0m func \u001B[38;5;241m=\u001B[39m stack[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m-> 1582\u001B[0m stack[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/torch/storage.py:520\u001B[0m, in \u001B[0;36m_load_from_bytes\u001B[0;34m(b)\u001B[0m\n\u001B[1;32m    519\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_from_bytes\u001B[39m(b):\n\u001B[0;32m--> 520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mBytesIO\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/torch/serialization.py:1384\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mUnpicklingError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1383\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mUnpicklingError(_get_wo_message(\u001B[38;5;28mstr\u001B[39m(e))) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1384\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_legacy_load\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1385\u001B[0m \u001B[43m    \u001B[49m\u001B[43mopened_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpickle_load_args\u001B[49m\n\u001B[1;32m   1386\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/torch/serialization.py:1638\u001B[0m, in \u001B[0;36m_legacy_load\u001B[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[1;32m   1636\u001B[0m unpickler \u001B[38;5;241m=\u001B[39m UnpicklerWrapper(f, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args)\n\u001B[1;32m   1637\u001B[0m unpickler\u001B[38;5;241m.\u001B[39mpersistent_load \u001B[38;5;241m=\u001B[39m persistent_load\n\u001B[0;32m-> 1638\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43munpickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1640\u001B[0m deserialized_storage_keys \u001B[38;5;241m=\u001B[39m pickle_module\u001B[38;5;241m.\u001B[39mload(f, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args)\n\u001B[1;32m   1642\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_guards\u001B[38;5;241m.\u001B[39mactive_fake_mode() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/torch/serialization.py:1566\u001B[0m, in \u001B[0;36m_legacy_load.<locals>.persistent_load\u001B[0;34m(saved_id)\u001B[0m\n\u001B[1;32m   1564\u001B[0m     obj \u001B[38;5;241m=\u001B[39m cast(Storage, torch\u001B[38;5;241m.\u001B[39mUntypedStorage(nbytes))\n\u001B[1;32m   1565\u001B[0m     obj\u001B[38;5;241m.\u001B[39m_torch_load_uninitialized \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m-> 1566\u001B[0m     obj \u001B[38;5;241m=\u001B[39m \u001B[43mrestore_location\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1567\u001B[0m \u001B[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001B[39;00m\n\u001B[1;32m   1568\u001B[0m \u001B[38;5;66;03m# stop wrapping with TypedStorage\u001B[39;00m\n\u001B[1;32m   1569\u001B[0m typed_storage \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstorage\u001B[38;5;241m.\u001B[39mTypedStorage(\n\u001B[1;32m   1570\u001B[0m     wrap_storage\u001B[38;5;241m=\u001B[39mobj, dtype\u001B[38;5;241m=\u001B[39mdtype, _internal\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1571\u001B[0m )\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/torch/serialization.py:601\u001B[0m, in \u001B[0;36mdefault_restore_location\u001B[0;34m(storage, location)\u001B[0m\n\u001B[1;32m    581\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001B[39;00m\n\u001B[1;32m    583\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    598\u001B[0m \u001B[38;5;124;03m       all matching ones return `None`.\u001B[39;00m\n\u001B[1;32m    599\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    600\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, _, fn \u001B[38;5;129;01min\u001B[39;00m _package_registry:\n\u001B[0;32m--> 601\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    602\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    603\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/torch/serialization.py:540\u001B[0m, in \u001B[0;36m_deserialize\u001B[0;34m(backend_name, obj, location)\u001B[0m\n\u001B[1;32m    538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m location\u001B[38;5;241m.\u001B[39mstartswith(backend_name):\n\u001B[1;32m    539\u001B[0m     device \u001B[38;5;241m=\u001B[39m _validate_device(location, backend_name)\n\u001B[0;32m--> 540\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/torch/storage.py:279\u001B[0m, in \u001B[0;36m_StorageBase.to\u001B[0;34m(self, device, non_blocking)\u001B[0m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mto\u001B[39m(\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m, device: torch\u001B[38;5;241m.\u001B[39mdevice, non_blocking: _bool \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    278\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[_StorageBase, TypedStorage]:\n\u001B[0;32m--> 279\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_to\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/hun-ren-fCnNvFWv-py3.12/lib/python3.12/site-packages/torch/_utils.py:88\u001B[0m, in \u001B[0;36m_to\u001B[0;34m(self, device, non_blocking)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[1;32m     86\u001B[0m         \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_sparse\n\u001B[1;32m     87\u001B[0m     ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse storage is not supported for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdevice\u001B[38;5;241m.\u001B[39mtype\u001B[38;5;241m.\u001B[39mupper()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m tensors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 88\u001B[0m     untyped_storage \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mUntypedStorage\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m     untyped_storage\u001B[38;5;241m.\u001B[39mcopy_(\u001B[38;5;28mself\u001B[39m, non_blocking)\n\u001B[1;32m     90\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m untyped_storage\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 368.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 384.00 KiB is free. Process 4938 has 1011.00 MiB memory in use. Including non-PyTorch memory, this process has 981.00 MiB memory in use. Of the allocated memory 918.40 MiB is allocated by PyTorch, and 25.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "source": [
    "model.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 Lekérjük a similarity hetmapet, hogy vizuálisan lássuk, mely témák hasonlóak"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.visualize_heatmap()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 Lekérjük a témák hierarchiáját, hogy lássuk a témák alá-fölé rendeltségének összefüggéseit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "hierarchical_topics = model.hierarchical_topics(doc_stop_2)\n",
    "\n",
    "# Visualize these representations\n",
    "model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 5.3 Megnézzük, mely témák hasonlítanak számszerűsítve",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "distance_matrix = cosine_similarity(np.array(model.topic_embeddings_))\n",
    "dist_df = pd.DataFrame(distance_matrix, columns=model.topic_labels_.values(),\n",
    "                       index=model.topic_labels_.values())\n",
    "\n",
    "tmp = []\n",
    "for rec in dist_df.reset_index().to_dict('records'):\n",
    "    t1 = rec['index']\n",
    "    for t2 in rec:\n",
    "        if t2 == 'index':\n",
    "            continue\n",
    "        tmp.append(\n",
    "            {\n",
    "                'topic1': t1,\n",
    "                'topic2': t2,\n",
    "                'distance': rec[t2]\n",
    "            }\n",
    "        )\n",
    "\n",
    "pair_dist_df = pd.DataFrame(tmp)\n",
    "\n",
    "pair_dist_df = pair_dist_df[(pair_dist_df.topic1.map(\n",
    "      lambda x: not x.startswith('-1'))) &\n",
    "            (pair_dist_df.topic2.map(lambda x: not x.startswith('-1')))]\n",
    "pair_dist_df = pair_dist_df[pair_dist_df.topic1 < pair_dist_df.topic2]\n",
    "pair_dist_df.sort_values('distance', ascending = False).head(20)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 5.4 Megnézzük, mely témák hasonlítanak nagyobb, mint 85%-ban. Lementjük az adatot.",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "similar_topics = pair_dist_df[pair_dist_df[\"distance\"] > 0.85]\n",
    "similar_topics"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "similar_topics.to_csv(\"../results/bert_probabilities_similarity_85.csv\")",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 5.5 Áttanulmányozzuk a lementett csv-t és eldöntjük, mely témákat érdemes összevonni a számszerűsített hasonlóság miatt. Megadjuk az összevonandó témákat, majd összevonjuk ezeket.",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "topics_to_merge = [[7,2,8,9,14,16,23,27,33,32,34,41,43,44,45], [1,6,36], [14,16,0,28,32,46,47,39,5,9 ], [20,22],[19,42]]\n",
    "model.merge_topics(doc_stop_2, topics_to_merge)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model.update_topics(doc_stop_2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "topics = model.topics_\n",
    "probabilities = model.probabilities_"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "probabilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "probs_df=pd.DataFrame(probabilities)\n",
    "probs_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "probs_df.to_csv(\"../results/prob_matrix.csv\", sep=\",\", encoding=\"UTF-8\", index=False)",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 5.6 Elnevezzük a topikokat",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "topic_labels = model.generate_topic_labels(nr_words=5,\n",
    "                                                 topic_prefix=False,\n",
    "                                                 word_length=10,\n",
    "                                                 separator=\", \")\n",
    "model.set_topic_labels(topic_labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model.set_topic_labels({0: \"diskurzuselem\", 1: \"helyeslés, hümmögés\", 2: \"nevetés\"})"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "topicinfo=model.get_topic_info()\n",
    "topicinfo"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 5.7 Lementjük a frissített, összevont és átnevezett témákat tartalmazó topic modellt és a topikokat",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": "save_topic_info(topicinfo, \"../results/bert_probabilities_merged_85_new_names.csv\")",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "save_model(model,\"../models/bert_probabilities_merged.pkl\")",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
